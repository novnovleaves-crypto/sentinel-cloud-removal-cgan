{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e5c8ef0",
   "metadata": {},
   "source": [
    "## 1 Importing Libs and Configuring Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce568c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from osgeo import gdal\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "\n",
    "DATA_FOLDER = \"cloud_removal_dataset_california\"\n",
    "OUTPUT_DATASET = \"dataset\"\n",
    "\n",
    "TRAIN_RATIO = 0.70\n",
    "VAL_RATIO = 0.15\n",
    "TEST_RATIO = 0.15\n",
    "\n",
    "MIN_IMAGE_SIZE = 512\n",
    "MAX_NODATA_RATIO_IN_CROP = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a42dddc",
   "metadata": {},
   "source": [
    "## 2 Define the processing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27b66394",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tif_as_array(file_path):\n",
    "    dataset = gdal.Open(file_path)\n",
    "    if dataset is None:\n",
    "        raise ValueError(f\"Cannot open {file_path}\")\n",
    "    \n",
    "    bands = []\n",
    "    for i in range(1, min(dataset.RasterCount + 1, 4)):\n",
    "        band = dataset.GetRasterBand(i)\n",
    "        band_array = band.ReadAsArray()\n",
    "        bands.append(band_array)\n",
    "    \n",
    "    image = np.stack(bands, axis=-1)\n",
    "    return image\n",
    "\n",
    "def create_common_valid_mask(img1, img2, threshold=0):\n",
    "    if img1.ndim == 3:\n",
    "        mask1 = np.any(img1 > threshold, axis=2)\n",
    "    else:\n",
    "        mask1 = img1 > threshold\n",
    "    \n",
    "    if img2.ndim == 3:\n",
    "        mask2 = np.any(img2 > threshold, axis=2)\n",
    "    else:\n",
    "        mask2 = img2 > threshold\n",
    "    \n",
    "    return mask1 & mask2\n",
    "\n",
    "def crop_to_common_valid_region(img1, img2):\n",
    "    common_mask = create_common_valid_mask(img1, img2, threshold=0)\n",
    "    \n",
    "    if not np.any(common_mask):\n",
    "        return None, None, \"no_valid_pixels\"\n",
    "    \n",
    "    rows = np.any(common_mask, axis=1)\n",
    "    cols = np.any(common_mask, axis=0)\n",
    "    \n",
    "    if not np.any(rows) or not np.any(cols):\n",
    "        return None, None, \"no_valid_rows_cols\"\n",
    "    \n",
    "    rmin, rmax = np.where(rows)[0][[0, -1]]\n",
    "    cmin, cmax = np.where(cols)[0][[0, -1]]\n",
    "    \n",
    "    cropped1 = img1[rmin:rmax+1, cmin:cmax+1]\n",
    "    cropped2 = img2[rmin:rmax+1, cmin:cmax+1]\n",
    "    \n",
    "    crop_height, crop_width = cropped1.shape[0], cropped1.shape[1]\n",
    "    \n",
    "    if crop_height < MIN_IMAGE_SIZE or crop_width < MIN_IMAGE_SIZE:\n",
    "        return None, None, f\"too_small_{crop_height}x{crop_width}\"\n",
    "    \n",
    "    cropped_mask = common_mask[rmin:rmax+1, cmin:cmax+1]\n",
    "    valid_pixels_in_crop = np.sum(cropped_mask)\n",
    "    total_pixels_in_crop = cropped_mask.size\n",
    "    nodata_ratio = 1 - (valid_pixels_in_crop / total_pixels_in_crop)\n",
    "    \n",
    "    if nodata_ratio > MAX_NODATA_RATIO_IN_CROP:\n",
    "        return None, None, f\"too_much_nodata_{nodata_ratio:.2f}\"\n",
    "    \n",
    "    return cropped1, cropped2, \"success\"\n",
    "\n",
    "def normalize_image_pair(cloudy_img, clear_img):\n",
    "    combined = np.concatenate([cloudy_img, clear_img], axis=0)\n",
    "    \n",
    "    valid_mask = np.any(combined > 0, axis=2)\n",
    "    valid_pixels = combined[valid_mask]\n",
    "    \n",
    "    p_low = np.percentile(valid_pixels, 2)\n",
    "    p_high = np.percentile(valid_pixels, 98)\n",
    "    \n",
    "    cloudy_norm = np.clip((cloudy_img - p_low) / (p_high - p_low) * 255, 0, 255).astype(np.uint8)\n",
    "    clear_norm = np.clip((clear_img - p_low) / (p_high - p_low) * 255, 0, 255).astype(np.uint8)\n",
    "    \n",
    "    return cloudy_norm, clear_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33fa994",
   "metadata": {},
   "source": [
    "## 3 data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b773555b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No existing dataset found. Starting preprocessing...\n",
      "\n",
      "Found 856 clear and 860 cloudy images\n",
      "Phase 1: Validating pairs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56a9526e451f4d71953ec62bb6f5718c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/856 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid pairs: 200, Skipped: 656\n",
      "Split: 139 train, 31 val, 30 test\n",
      "\n",
      "Phase 2: Processing and saving images in parallel...\n",
      "Using 8 parallel workers\n",
      "\n",
      "Processing train set (139 images)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c011ad94cb3349e6bb499f16a5c5d70e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/139 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saved 139/139 images\n",
      "\n",
      "Processing val set (31 images)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bac9cb5513294c61b58bd891b422dd65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "val:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saved 31/31 images\n",
      "\n",
      "Processing test set (30 images)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3420b580ffa041be915a758176b8d391",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saved 30/30 images\n",
      "\n",
      "Total saved: 200 images\n"
     ]
    }
   ],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "\n",
    "train_files = glob.glob(os.path.join(OUTPUT_DATASET, \"train\", \"*.png\"))\n",
    "val_files = glob.glob(os.path.join(OUTPUT_DATASET, \"val\", \"*.png\"))\n",
    "test_files = glob.glob(os.path.join(OUTPUT_DATASET, \"test\", \"*.png\"))\n",
    "\n",
    "if train_files or val_files or test_files:\n",
    "    print(f\"Dataset already exists:\")\n",
    "    print(f\"  Train: {len(train_files)} images\")\n",
    "    print(f\"  Val: {len(val_files)} images\")\n",
    "    print(f\"  Test: {len(test_files)} images\")\n",
    "    print(\"\\nSkipping preprocessing. Delete the 'dataset' folder to reprocess.\")\n",
    "else:\n",
    "    print(\"No existing dataset found. Starting preprocessing...\\n\")\n",
    "    \n",
    "    os.makedirs(os.path.join(OUTPUT_DATASET, \"train\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(OUTPUT_DATASET, \"val\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(OUTPUT_DATASET, \"test\"), exist_ok=True)\n",
    "    \n",
    "    clear_files = sorted(glob.glob(os.path.join(DATA_FOLDER, \"clear_*.tif\")))\n",
    "    cloudy_files = sorted(glob.glob(os.path.join(DATA_FOLDER, \"cloudy_*.tif\")))\n",
    "    \n",
    "    print(f\"Found {len(clear_files)} clear and {len(cloudy_files)} cloudy images\")\n",
    "    \n",
    "    pair_indices = []\n",
    "    skip_reasons = {}\n",
    "    \n",
    "    print(\"Phase 1: Validating pairs...\")\n",
    "    for i in tqdm(range(min(len(clear_files), len(cloudy_files)))):\n",
    "        clear_path = os.path.join(DATA_FOLDER, f\"clear_{i}.tif\")\n",
    "        cloudy_path = os.path.join(DATA_FOLDER, f\"cloudy_{i}.tif\")\n",
    "        \n",
    "        if not (os.path.exists(clear_path) and os.path.exists(cloudy_path)):\n",
    "            skip_reasons['file_not_found'] = skip_reasons.get('file_not_found', 0) + 1\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            cloudy_raw = load_tif_as_array(cloudy_path)\n",
    "            clear_raw = load_tif_as_array(clear_path)\n",
    "            \n",
    "            cloudy_cropped, clear_cropped, reason = crop_to_common_valid_region(cloudy_raw, clear_raw)\n",
    "            \n",
    "            if cloudy_cropped is None:\n",
    "                skip_reasons[reason] = skip_reasons.get(reason, 0) + 1\n",
    "                continue\n",
    "            \n",
    "            pair_indices.append(i)\n",
    "                \n",
    "        except Exception as e:\n",
    "            skip_reasons['exception'] = skip_reasons.get('exception', 0) + 1\n",
    "            continue\n",
    "    \n",
    "    print(f\"Valid pairs: {len(pair_indices)}, Skipped: {sum(skip_reasons.values())}\")\n",
    "    \n",
    "    if len(pair_indices) < 10:\n",
    "        raise ValueError(\"Too few valid pairs for training\")\n",
    "    \n",
    "    train_val_idx, test_idx = train_test_split(pair_indices, test_size=TEST_RATIO, random_state=42)\n",
    "    relative_val_size = VAL_RATIO / (TRAIN_RATIO + VAL_RATIO)\n",
    "    train_idx, val_idx = train_test_split(train_val_idx, test_size=relative_val_size, random_state=42)\n",
    "    \n",
    "    split_mapping = {}\n",
    "    for idx in train_idx:\n",
    "        split_mapping[idx] = 'train'\n",
    "    for idx in val_idx:\n",
    "        split_mapping[idx] = 'val'\n",
    "    for idx in test_idx:\n",
    "        split_mapping[idx] = 'test'\n",
    "    \n",
    "    print(f\"Split: {len(train_idx)} train, {len(val_idx)} val, {len(test_idx)} test\")\n",
    "    \n",
    "    print(\"\\nPhase 2: Processing and saving images in parallel...\")\n",
    "    \n",
    "    def process_single_pair(i, split, counter_start):\n",
    "        try:\n",
    "            clear_path = os.path.join(DATA_FOLDER, f\"clear_{i}.tif\")\n",
    "            cloudy_path = os.path.join(DATA_FOLDER, f\"cloudy_{i}.tif\")\n",
    "            \n",
    "            cloudy_raw = load_tif_as_array(cloudy_path)\n",
    "            clear_raw = load_tif_as_array(clear_path)\n",
    "            \n",
    "            cloudy_cropped, clear_cropped, _ = crop_to_common_valid_region(cloudy_raw, clear_raw)\n",
    "            \n",
    "            cloudy_norm, clear_norm = normalize_image_pair(cloudy_cropped, clear_cropped)\n",
    "            \n",
    "            combined = np.concatenate([cloudy_norm, clear_norm], axis=1)\n",
    "            \n",
    "            combined_pil = Image.fromarray(combined)\n",
    "            output_path = os.path.join(OUTPUT_DATASET, split, f\"pair_{counter_start:04d}.png\")\n",
    "            combined_pil.save(output_path)\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            return False\n",
    "    \n",
    "    splits_to_process = [\n",
    "        ('train', [i for i in pair_indices if split_mapping[i] == 'train']),\n",
    "        ('val', [i for i in pair_indices if split_mapping[i] == 'val']),\n",
    "        ('test', [i for i in pair_indices if split_mapping[i] == 'test'])\n",
    "    ]\n",
    "    \n",
    "    n_jobs = min(multiprocessing.cpu_count() - 1, 8)\n",
    "    print(f\"Using {n_jobs} parallel workers\")\n",
    "    \n",
    "    total_saved = 0\n",
    "    \n",
    "    for split_name, indices in splits_to_process:\n",
    "        print(f\"\\nProcessing {split_name} set ({len(indices)} images)...\")\n",
    "        \n",
    "        results = Parallel(n_jobs=n_jobs)(\n",
    "            delayed(process_single_pair)(idx, split_name, counter) \n",
    "            for counter, idx in enumerate(tqdm(indices, desc=split_name))\n",
    "        )\n",
    "        \n",
    "        saved = sum(results)\n",
    "        total_saved += saved\n",
    "        print(f\"  Saved {saved}/{len(indices)} images\")\n",
    "    \n",
    "    print(f\"\\nTotal saved: {total_saved} images\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
